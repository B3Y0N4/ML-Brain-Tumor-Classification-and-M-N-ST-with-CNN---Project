{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26135,"status":"ok","timestamp":1678450708543,"user":{"displayName":"Jalal Jr","userId":"08481911706047051466"},"user_tz":-180},"id":"7NNilYw9crbv","outputId":"730e4ba4-77a5-420f-9bdc-2caa17f76bae"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /gdrive\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"LBkhH9ORdAcF"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 3264 files belonging to 2 classes.\n","Using 2612 files for training.\n","Found 3264 files belonging to 2 classes.\n","Using 652 files for validation.\n","Epoch 1/10\n","82/82 [==============================] - 1018s 11s/step - loss: 3.8640 - accuracy: 0.8170 - val_loss: 9.6967 - val_accuracy: 0.1503\n","Epoch 2/10\n","82/82 [==============================] - 649s 8s/step - loss: 0.4895 - accuracy: 0.8453 - val_loss: 0.5179 - val_accuracy: 0.7991\n","Epoch 3/10\n","82/82 [==============================] - 639s 8s/step - loss: 0.3698 - accuracy: 0.8595 - val_loss: 0.3196 - val_accuracy: 0.9034\n","Epoch 4/10\n","82/82 [==============================] - 630s 8s/step - loss: 0.3380 - accuracy: 0.8679 - val_loss: 0.3199 - val_accuracy: 0.9064\n","Epoch 5/10\n","82/82 [==============================] - 624s 8s/step - loss: 0.3528 - accuracy: 0.8710 - val_loss: 0.3439 - val_accuracy: 0.9018\n","Epoch 6/10\n","82/82 [==============================] - 622s 8s/step - loss: 0.3563 - accuracy: 0.8691 - val_loss: 0.4282 - val_accuracy: 0.9018\n","Epoch 7/10\n","82/82 [==============================] - 620s 8s/step - loss: 0.3396 - accuracy: 0.8683 - val_loss: 0.2735 - val_accuracy: 0.9018\n","Epoch 8/10\n","59/82 [====================\u003e.........] - ETA: 2:43 - loss: 0.2948 - accuracy: 0.8755"]}],"source":["# Import the necessary libraries\n","import tensorflow as tf\n","from tensorflow.keras import layers, Model\n","\n","# Define the image size and batch size\n","image_size = (256, 256)\n","batch_size = 32\n","\n","# Define the path to your data directory in your drive\n","data_dir = '/gdrive/My Drive/brainTumorClassification'\n","\n","# Use tf.keras.preprocessing.image_dataset_from_directory() to load the data\n","train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","  data_dir,\n","  validation_split=0.2,\n","  subset=\"training\",\n","  seed=42,\n","  image_size=image_size,\n","  batch_size=batch_size\n",")\n","\n","val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","  data_dir,\n","  validation_split=0.2,\n","  subset=\"validation\",\n","  seed=42,\n","  image_size=image_size,\n","  batch_size=batch_size\n",")\n","\n","# Define the AlexNet model architecture\n","inputs = layers.Input(shape=(256, 256, 3))\n","\n","x = layers.Conv2D(96, 11, strides=4, activation='relu')(inputs)\n","x = layers.BatchNormalization()(x)\n","x = layers.MaxPooling2D(3, strides=2)(x)\n","\n","x = layers.Conv2D(256, 5, strides=1, padding='same', activation='relu')(x)\n","x = layers.BatchNormalization()(x)\n","x = layers.MaxPooling2D(3, strides=2)(x)\n","\n","x = layers.Conv2D(384, 3, strides=1, padding='same', activation='relu')(x)\n","x = layers.BatchNormalization()(x)\n","\n","x = layers.Conv2D(384, 3, strides=1, padding='same', activation='relu')(x)\n","x = layers.BatchNormalization()(x)\n","\n","x = layers.Conv2D(256, 3, strides=1, padding='same', activation='relu')(x)\n","x = layers.BatchNormalization()(x)\n","x = layers.MaxPooling2D(3, strides=2)(x)\n","\n","x = layers.Flatten()(x)\n","x = layers.Dense(4096, activation='relu')(x)\n","x = layers.Dropout(0.5)(x)\n","x = layers.Dense(4096, activation='relu')(x)\n","x = layers.Dropout(0.5)(x)\n","outputs = layers.Dense(1, activation='sigmoid')(x)\n","\n","model = Model(inputs, outputs)\n","\n","# Compile the model\n","model.compile(optimizer='adam',\n","              loss=tf.losses.BinaryCrossentropy(),\n","              metrics=['accuracy'])\n","\n","# Fit the model to the data\n","model.fit(\n","  train_ds,\n","  validation_data=val_ds,\n","  epochs=10\n",")\n","\n","# Evaluate the model on the testing data\n","test_loss, test_acc = model.evaluate(val_ds, verbose=2)\n","print('Test accuracy:', test_acc)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPZgGMbYfmm3AAIwwD5KQA1","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}